#Piloting Learning - Assessment & Adaptation

By sticking rigidly to a script, whether it's a textbook, a set of notes or a lesson plan, we are essentially planning ourselves into a corner; that process relies on us being able to make reliable assumptions of our learners' knowledge, relies on our learners all having very similar needs so they can be served with the same lesson, and relies on us skillfully constructing what an adequate delivery technique will be. Instead, we want to conduct a more adaptive lesson; one that focuses on identifying & addressing our learners' misconceptions and gaps in knowledge in situ, and gives them multiple distinct opportunities to construct knowledge and understanding accurately and for themselves.

##Summative & Formative Assessment

Any adaptive process requires observations to adapt to - so, we need effective tools to observe & assess student understanding. Student assessment is commonly grouped into two gross categories: *summative*, and *formative*.

**Summative Assessment** is intended to measure learning outcomes; the canonical example is the final exam. A well-designed summative assessment should reveal how well students have met the design goals of a course or lesson, as laid out in the first step of reverse instructional design; its principle use is for deciding if the course as a whole is satisfying its intended purpose, typically for performance review of the educational process at 'global scope'.

**Formative Assessment** is intended to measure student understanding or ability in situ during a course, in order to adapt the course on the fly to optimise the efficacy of that course.

Superficially, these sound pretty similar, up to when exactly they are administered; they both assess student understanding & performance. The key difference between the two is *resolution*; in essence, summative assessment is only required to provide a simple up or down indicator of whether a course did its job or not; but formative assessment has to not only flag gaps in student performance, but resolve *why and how* students are missing the mark, in order to optimally inform instructional course correction.

Institutional western education has converged largely on summative assessment - principally because it's cheaper and easier. Summative assessment can be actualized in standardized testing, which can be administered uniformly at industrial, mass-production scale, with minimal interruption to already cramped classroom time, and responded to off line in future iterations of a course, as planning time and budget permit. But formative assessment is far more personal, deeper, and operational in real-time; it must be conducted on a per-classroom basis, typically during class time, and appropriate course corrections must be available at the instructor's fingertips. The scale that leads to summative assessment's efficiency is also its greatest weakness; differences between individual classes and cohorts get washed out, and adaptation can only converge toward a global mean. Formative assessment, though more labor intensive, is also far more nimble.

(see activity assessment_01.html)



/////////////////////////
MCQ == formalization of socratic questioning
q: why am I up here?
a: to teach you stuff
q: what is the most important word in 'teach you stuff'
a: 'you'. most people think its 'teach', leads to impersonal one-size march through teaching. the teaching process works best when it considers its students.
